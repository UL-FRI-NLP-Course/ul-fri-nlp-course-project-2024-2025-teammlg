%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{natbib}
\usepackage{acro}
\graphicspath{{fig/}}
\usepackage{xcolor}
\usepackage{tcolorbox}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}



%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Conversational Agent with Retrieval-Augmented Generation} 

% Authors (student competitors) and their info
\Authors{Matej Belšak, Gorazd Gorup, Luka Bajić}

% Advisors
\affiliation{\textit{Advisors: Aleš Žagar}}

% Keywords
\Keywords{Conversational agent, Retrieval-Augmented Generation}
\newcommand{\keywordname}{Keywords}


% Abbreviations
\DeclareAcronym{llm}{
	short=LLM,
	long=Large Language Model
}

\DeclareAcronym{rag}{
	short=RAG,
	long=Retrieval Augmented Generation
}

\DeclareAcronym{nlp}{
	short=NLP,
	long=Natural Language Processing
}

\DeclareAcronym{tmdb}{
	short=TMDB,
	long=The Movie Database
}

\newcommand{\etal}{\textit{et al}., }
\newcommand{\ie}{\textit{i}.\textit{e}., }
\newcommand{\eg}{\textit{e}.\textit{g}.\ }

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
Develop a conversational agent that enhances the quality and accuracy of its responses by dynamically retrieving and integrating relevant external documents from the web. Unlike traditional chatbots that rely solely on pre-trained knowledge, this system will perform real-time information retrieval, ensuring up-to-date answers. Potential applications include customer support, academic research assistance and general knowledge queries. The project will involve natural language processing, web scraping, and retrieval-augmented generation techniques to optimize answer quality.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
	
While \acp{llm} have evolved considerably and now produce convincing replies, they have inherent limitations. They rely on training data consisting of documents from the past and may not possess knowledge of current events and developments. Due to differences and properties of training datasets, they may not contain specific domain knowledge, failing to answer certain prompts or outright hallucinating. The latter could prove especially disastrous in dedicated chatbots, for example for legal guidance or health care~\cite{thirunavukarasu2023large}.

One possibility would be to retrain the chat model in intervals to try to keep it up to date, but that would still produce periods where the information is missing from the \ac{llm} or is outdated. This approach, while completelly time and energy inefficient, would also not guard against hallucinations. To solve these issues, \ac{rag} is used to provide the missing knowledge to the \ac{llm}. \ac{rag} employs different techniques to retrieve information from external sources based on user’s prompt and through prompt augmentation feed the \ac{llm} sufficient information to provide informative and factually correct answer. In the survey by Gao~\etal~\cite{survey}, multiple approaches to \ac{rag} are presented, highlighting three architectures: naive \ac{rag}, which analyzes the user’s prompt, retrieves the required information and appends it, letting the \ac{llm} do the rest; advanced \ac{rag}, which employs pre-retrieval and post-retrieval modifications to the prompt to make it more suitable for information retrieval and subsequent interpretation by \ac{llm}; lastly, modular \ac{rag} combines multiple approaches, using iterative prompt enhancement, ranking, fusion, etc.

To better evaluate \ac{rag} performance, Lyu~\etal~\cite{benchmark} describe the CRUD framework, employing metrics such as ROUGE, BLEU, precision and recall. Various operations on text (creative generation from context, usage of information to answer questions, identification and correction of false information, summarization, ...) are measured separately to give a more detailed overview of the model. 

For document summarization, \acp{llm}, statistical models, graph-based models and other approaches are used to extract the most important information from text. Zhang~\etal~\cite{summarization} present multiple solutions, noting that \acp{llm}, while consuming more resources, tend to be more coherent and precise in their summarization if trained correctly.

In this paper, we focus on \ac{rag} methods and their use in chatbots. To that end, we design a conversational agent operating on knowledge about different art and media. Specifically, the agent is to suggest and converse about films and other related media based on the user’s prompts and preferences. Our contributions are:

\begin{itemize}
	\item Implementation of a conversational agent using two different pretrained models: DeepSeek-R1~\cite{deepseek3} and Qwen 3~\cite{qwen3}.
	\item Implementation of two \ac{rag} techniques, a primitive and advanced one, with capabilities of retrieving data from various film-related databases and web sources.
	\item Evaluation of conversational agents with respect to the model used and the \ac{rag} technique. Agents are evaluated against the baseline \ac{rag}-less chatbots. Human and \ac{llm} judges are used to score responses, and different metrics are analyzed.
\end{itemize}

%------------------------------------------------

\section*{Methods}

% TODO: A schematic of our pipeline

\subsection*{Models}

% TODO: Deepseek \footnote{https://huggingface.co/deepseek-ai/DeepSeek-R1}

Recently, a novel LLM, DeepSeek \cite{deepseek3}, has been presented. Because of its positive benchmarking results and efficiency due to the small number of parameters, it provides a promising starting point for experimentation with knowledge injection and prompt engineering.

% TODO: Qwen

\subsection*{Information Retrieval}

% TODO: Sources: TMDB, JustWatch, Letterboxd, Wikipedia

% TODO: Which data, how much, what format

Crucial step in the RAG pipeline is the gathering of data (that the model has not been trained on) from external sources, in our case this is achieved via web scraping. Based on the user prompt, our models can access the following data:
\begin{itemize}
	\item Basic information about a film or a person, obtained from TMDB.
	\item A specified number of film reviews, obtained from social media platform Letterboxd, sorted by popularity.
	\item A list of streaming services a specific film is available on.
	\item Summaries and plot descriptions from Wikipedia pages.
\end{itemize}

\subsection*{\ac{rag}}

% TODO: Injection into prompt - what format, how much
% Prepend certain info: e.g. date

% TODO: What are prompt limits (length, tokens...)?

% TODO: Simple RAG
% 		- POS tagging
%		- prompt injection

For the purpose of identifying film titles or names in user's prompt and form summarization of retrieved data later on, we use a Roberta-like spacy model~\footnote{https://huggingface.co/spacy/en\_core\_web\_trf}.

% TODO: Advanced RAG v1
%		- POS tagging
% 		- info extraction
%		- summarization
%		- performance - disadvantages

% TODO: Advanced RAG v2
%		- let LLM decide what to retrieve
%		- function calling
%		- prompt construction

%------------------------------------------------

\section*{Results}

In this section we compare the performance of a reasoning DeepSeek-R1-Distill-Llama-8B \cite{deepseek3} model and a non-reasoning Qwen3-8B \cite{qwen3} model. For each of these we run experiments on three different sets of parameters: without RAG, with naive RAG - data is scraped and appended to the context, and advanced RAG - data is additionally processed and summarized before being inputed into the model. In total, we are evaluating the following models:
\begin{itemize}
	\item DeepSeek-R1-Distill-Llama-8B (baseline)
	\item DeepSeek-R1-Distill-Llama-8B with naive RAG
	\item DeepSeek-R1-Distill-Llama-8B with advanced RAG
	\item Qwen3-8B (baseline)
	\item Qwen3-8B with naive RAG
	\item Qwen3-8B with advanced RAG
\end{itemize}

Since we are working with relatively open-ended questions, there is a lack of objective ground truth to evaluate against. Therefore, we construct a set of 50 domain-relevant questions and use manually checked answers from the commercial ChatGPT model as ground truth, so we can apply standard metrics such as ROUGE and BLEU \footnote{https://huggingface.co/docs/evaluate/index}. To avoid overfitting a model to a specific set of questions, we included multiple types of questions into our test set, including yes/no questions, fact checking, list retrieval and summarization. Results are shown in Table \ref{tab:comparison}.

\begin{table}[!htb]
\centering
\resizebox{1.0 \linewidth}{!}{\begin{tabular}{| l | c c |}
\hline
Model & ROUGE & BLEU   \\ \hline
deepseek-baseline  & 0.171003 & 0.049453 \\
deepseek-naive     & 0.165164 & 0.043081   \\
deepseek-advanced  & 0.16714  & 0.01866   \\
qwen-baseline      & 0.192379 & 0.031395    \\
qwen-naive         & 0.194950 & 0.034025  \\
qwen-advanced      & 0.17380  & 0.02584   \\
\hline
\end{tabular}}
\caption{Performance comparison against ChatGPT-generated ground truth.}
\label{tab:comparison}
\end{table}


\subsection*{LLM-based evaluation}

Metrics such as BLEU and ROUGE are ill-adjusted to our test set, therefore we employ the DeepEval framework \cite{deepeval} in order to obtain more relevant metrics based on context, query and response. Results are shown in Table \ref{tab:llm_metrics}. Metrics related to context are not available for baseline models, because they do not receive any context. 

\begin{table*}[!htb]
%\centering
\resizebox{1.0 \linewidth}{!}{\begin{tabular}{| c | c c c c c c c |}
\hline
model & correctness & clarity & answer relevancy & faithfulness & contextual precision & contextual recall & contextual relevancy \\ \hline
deepseek-baseline  & 0.19 & 0.7725 & 0.76846 & \textbf{0.98615} & / & / & / \\
deepseek-naive  & 0.152 & 0.82800 & 0.78138 & 0.93233 & 0.24000 & 0.61771 & \textbf{0.40468}   \\
deepseek-advanced  & & & & & & &   \\
qwen-baseline  & \textbf{0.30962} & \textbf{0.86888} & 0.74805 & 0.94456 & / & / & /  \\
qwen-naive  & 0.23111 & 0.65888 & \textbf{0.84670} & 0.88345 & \textbf{0.88345} & \textbf{0.76333} & 0.37029  \\
qwen-advanced  &  & & &  &  & &    \\
\hline
\end{tabular}}
\caption{Performance comparison as evaluated by a 14B parameter Qwen model with GEval framework.}
\label{tab:llm_metrics}
\end{table*}


\subsection*{Conversational chatbot}

Extending a working model into a conversational chatbot is straightforward: the data retrieval pipeline remains the same, we only need to implement a mechanism for memorization of previous questions and responses. Examples of Qwen chatbot results are shown in Appendix \ref{qwenconvo}.


%------------------------------------------------

\section*{Discussion}

We conclude that it is possible to improve the responses of 8B parameter large language models on queries from a specific domain by employing on-the-fly information retrieval via web scraping.  



%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}

\newpage
\begin{appendix}

\section*{Appendix}
\label{append}

\subsection*{Qwen conversation example}

\begin{figure}
\begin{tcolorbox}
User: How many films did Chuck Jones make?

Qwen: Based on the data provided:

	**Total Number of Films:**  
	Chuck Jones directed **over 100 films**, spanning multiple genres and formats.

	\#\#\# Breakdown:
	- **Feature-Length Films**: 
    		- *The Bugs Bunny/Road Runner Movie* (1979)
	    - *The Phantom Tollbooth* (1970)
	    - *The Grinch Stole Christmas!* (1966)

	- **Shorts and Documentaries**:
	    - Approximately **100+ short cartoons**
	        - Many of these are Looney Tunes/Merry Melodies cartoons.
	        - Includes iconic titles such as *What's Opera, Doc?*, *Duck Amuck*, etc.
    
	    - **Documentaries**: Around **dozens**, e.g.,
	    	    - *The Magical World of Chuck Jones*
	        - *Chuck Jones: Memories of Childhood*

	\#\#\# Summary:
	Chuck Jones made approximately \textbf{more than 100 films}, primarily consisting of **\green{animated short cartoons}**, a handful of **\green{feature-length compilations}**, and several **\red{documentaries}**.
\end{tcolorbox}
\caption{Conversation with Qwen}
\label{qwenconvo}
\end{figure}


 


\end{appendix}


\end{document}