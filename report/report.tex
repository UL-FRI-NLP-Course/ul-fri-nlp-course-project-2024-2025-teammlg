%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Conversational Agent with Retrieval-Augmented Generation} 

% Authors (student competitors) and their info
\Authors{Matej Belšak, Gorazd Gorup, Luka Bajić}

% Advisors
\affiliation{\textit{Advisors: Aleš Žagar}}

% Keywords
\Keywords{Conversational agent, Retrieval-Augmented Generation}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
Develop a conversational agent that enhances the quality and accuracy of its responses by dynamically retrieving and integrating relevant external documents from the web. Unlike traditional chatbots that rely solely on pre-trained knowledge, this system will perform real-time information retrieval, ensuring up-to-date answers. Potential applications include customer support, academic research assistance and general knowledge queries. The project will involve natural language processing (NLP), web scraping, and retrieval-augmented generation (RAG) techniques to optimize answer quality.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
	
While \textit{Large Language Models} (LLMs) have evolved considerably and now produce convincing replies, they have inherent limitations. They rely on training data consisting of documents from the past and may not possess knowledge of current events and developments. Due to differences and properties of training datasets, they may not contain specific domain knowledge, failing to answer certain prompts or outright hallucinating. 


%------------------------------------------------

\section*{Methods}

To solve these issues, \textit{Retrieval-Augmented Generation} (RAG) is used to provide the missing knowledge to the LLM. RAG employs different techniques to retrieve information from external sources based on user’s prompt and through prompt augmentation feed the LLM sufficient information to provide informative and factually correct answer. In the survey \cite{survey}, multiple approaches to RAG are presented, highlighting three architectures: naive RAG, which analyzes the user’s prompt, retrieves the required information and appends it, letting the LLM do the rest; advanced RAG, which employs pre-retrieval and post-retrieval modifications to the prompt to make it more suitable for information retrieval and subsequent interpretation by LLM; lastly, modular RAG combines multiple approaches, using iterative prompt enhancement, ranking, fusion, etc. 


To better evaluate RAG performance, \cite{benchmark} presents the CRUD framework, employing metrics such as ROUGE, BLEU, precision and recall. Various operations on text (creative generation from context, usage of information to answer questions, identification and correction of false information, summarization, ...) are measured separately to give a more detailed overview of the model. 

For document summarization, LLMs, statistical models, graph-based models and other approaches are used to extract the most important information from text. \cite{summarization} present multiple solutions, noting that LLMs, while consuming more resources, tend to be more coherent and precise in their summarization if trained correctly. 

Recently, a novel LLM, DeepSeek \cite{deepseek3}, has been presented. Because of its positive benchmarking results and efficiency due to the small number of parameters, it provides a promising starting point for experimentation with knowledge injection and prompt engineering. 

\subsection*{Topic domain}
In this paper, we focus on implementing a conversational agent operating on knowledge about different art and media. Specifically, the agent is to suggest and converse about films and other related media on the user’s prompts and preferences. In our contributions, we: 
\begin{itemize}
\item Develop a conversational wrapper around an existing pretrained LLM, DeepSeek-R1 \footnote{https://huggingface.co/deepseek-ai/DeepSeek-R1}; 
\item Analyze and test prompt engineering techniques on LLM inputs for our defined use cases, noting the placement of information in the prompt, structuring of the prompt, and wording that produces best results; 
\item Implement an advanced and/or modular RAG to transform and enhance user prompts and inject necessary knowledge into the final prompt, using approaches such as summarization, ranking, iterative prompt enhancement, and sentiment analysis via smaller pretrained LLMs; 
\item Retrieve data from open databases, such as The\-Movie\-Database (TMDB) \footnote{https://www.themoviedb.org/}, JustWatch \footnote{https://www.justwatch.com/}, and social media platforms, e.g., Letterboxd \footnote{https://letterboxd.com/}; 
\item Perform benchmarks of our solution with CRUD framework; 
\item Compare our solution with advanced commercial LLMs, such as ChatGPT. 
\end{itemize}

\subsection*{Approach}
To develop a reliable conversational agent, we must address several challenges.
Our approach focuses on the following areas:
\begin{itemize}
\item Understanding user prompts:\\
Natural language queries are often ambiguous or incomplete, making effective information retrieval difficult.
\item Consistency:\\
Conversational agents must maintain logical coherence and avoid contradictions.


\end{itemize}


\subsection*{Data scraping}

Crucial step in the RAG pipeline is the gathering of data (that the model has not been trained on) from external sources, in our case this is achieved via web scraping. Based on the user prompt, our models can access the following data:
\begin{itemize}
	\item basic information about a film or a person, obtained from TMDB
	\item a specified number of film reviews, obtained from social media platform Letterboxd, sorted by popularity
	\item a list of streaming services a specific film is available on
\end{itemize}

\subsection*{POS tagging and summarization}

For the purpose of identifying film titles or names in user's prompt, we use a Roberta-like spacy model en\_core\_web\_trf \footnote{https://huggingface.co/spacy/en\_core\_web\_trf}. We use the same model to summarize retrieved data for advanced RAG.  


\subsection*{Conversational chatbot}




%------------------------------------------------

\section*{Results}

In this section we compare the performance of a reasoning DeepSeek-R1-Distill-Llama-8B \cite{deepseek3} model and a non-reasoning Qwen3-8B \cite{qwen3} model. For each of these we run experiments on three different sets of parameters: without RAG, with naive RAG - data is scraped and appended to the context, and advanced RAG - data is additionally processed and summarized before being inputed into the model. In total, we are evaluating the following models:
\begin{itemize}
	\item DeepSeek-R1-Distill-Llama-8B (baseline)
	\item DeepSeek-R1-Distill-Llama-8B with naive RAG
	\item DeepSeek-R1-Distill-Llama-8B with advanced RAG
	\item Qwen3-8B (baseline)
	\item Qwen3-8B with naive RAG
	\item Qwen3-8B with advanced RAG
\end{itemize}

Since we are working with relatively open-ended questions, there is a lack of objective ground truth to evaluate against. Therefore, we construct a set of 50 domain-relevant questions and use manually checked answers from the commercial ChatGPT model as ground truth, so we can apply standard metrics such as ROUGE and BLEU \footnote{https://huggingface.co/docs/evaluate/index}. To avoid overfitting a model to a specific set of questions, we included multiple types of questions into our test set, including yes/no questions, fact checking, list retrieval and summarization. Results are shown in Table \ref{tab:comparison}.

\begin{table}[!htb]
\centering
\resizebox{1.0 \linewidth}{!}{\begin{tabular}{| l | c c |}
\hline
% & &\multicolumn{5}{c}{Number of segmented training images}\\
Model & ROUGE & BLEU   \\ \hline
deepseek-baseline  & 0.13079 & 0.00567 \\
deepseek-naive  & 0.14052  & 0.01068   \\
deepseek-advanced  & 0.16714  & 0.01866   \\
qwen-baseline  & 0.18991 & 0.03687    \\
qwen-naive  & 0.19899  & 0.05811  \\
qwen-advanced  & 0.17380  & 0.02584   \\
\hline
\end{tabular}}
\caption{Performance comparison against ChatGPT-generated ground truth.}
\label{tab:comparison}
\end{table}


\subsection*{LLM-based evaluation}

Metrics such as BLEU and ROUGE are ill-adjusted to our test set, therefore we employ the DeepEval framework \cite{deepeval} in order to obtain more relevant metrics based on context, query and response. Results are shown in Table \ref{tab:llm_metrics}.

\begin{table*}[!htb]
%\centering
\resizebox{1.0 \linewidth}{!}{\begin{tabular}{| c | c c c c c c c |}
\hline
model & correctness & clarity & answer relevancy & faithfulness & contextual precision & contextual recall & contextual relevancy \\ \hline
deepseek-baseline  & 0.13 & 0.05 & 0.13 & 0.05 & / & / & / \\
deepseek-naive  & 0.13 & 0.05 & 0.13 & 0.05 & 0.13 & 0.05 & 0.05   \\
deepseek-advanced  & 0.13 & \textbf{0.13} & 0.13 & \textbf{0.13} & 0.13 & \textbf{0.13} & 0.05   \\
qwen-baseline  & 0.13 & 0.05 & 0.13 & 0.05 & / & / & /    \\
qwen-naive  & 0.13 & 0.05 & 0.13 & 0.05 & 0.13 & 0.05 & 0.05  \\
qwen-advanced  & \textbf{0.13} & 0.05 & \textbf{0.13} & 0.05 & \textbf{0.13} & 0.05 & \textbf{0.13}   \\
\hline
\end{tabular}}
\caption{Comparison of all models with LLM metrics.}
\label{tab:llm_metrics}
\end{table*}


%------------------------------------------------

\section*{Discussion}

At the moment, there is a visible improvement in the models' performance when using RAG, however the BLEU and ROUGE evaluation metrics do not emphasize this improvement enough, therefore we intend to devise more representative evaluation methods for our specific domain. Other open problems are how to determine which title/person a query is referring to in certain ambiguous cases and how to enable the model to keep a memory in the conversational context. 



%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\begin{appendix}

\section*{Appendix}
\label{append}

\subsection*{DeepSeek}

\fbox{\begin{minipage}{23em}
 "query": "Can you summarize the main themes of dark knight for me?",
        "reply": "The Dark Knight is a superhero film directed by Christopher Nolan. The primary themes of the film include:1. Justice and morality: Batman's fight against crime is seen as a struggle for justice, morality, and order in Gotham City.2. Power and corruption: Batman's battle against corruption is seen as a fight between power and corruption, as well as between good and evil.3. Individuality and heroism: The film also explores the idea of individuality and heroism, as Batman himself is not just a symbol of justice but also an individual with unique qualities and experiences that make him a true hero.", 
\end{minipage}}


 


\end{appendix}


\end{document}