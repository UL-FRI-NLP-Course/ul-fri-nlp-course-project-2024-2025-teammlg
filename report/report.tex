%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Conversational Agent with Retrieval-Augmented Generation} 

% Authors (student competitors) and their info
\Authors{Matej Belšak, Gorazd Gorup, Luka Bajić}

% Advisors
\affiliation{\textit{Advisors: Aleš Žagar}}

% Keywords
\Keywords{Conversational agent, Retrieval-Augmented Generation}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
Develop a conversational agent that enhances the quality and accuracy of its responses by dynamically retrieving and integrating relevant external documents from the web. Unlike traditional chatbots that rely solely on pre-trained knowledge, this system will perform real-time information retrieval, ensuring up-to-date answers. Potential applications include customer support, academic research assistance and general knowledge queries. The project will involve natural language processing (NLP), web scraping, and retrieval-augmented generation (RAG) techniques to optimize answer quality.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
	
While \textit{Large Language Models} (LLMs) have evolved considerably and now produce convincing replies, they have inherent limitations. They rely on training data consisting of documents from the past and may not possess knowledge of current events and developments. Due to differences and properties of training datasets, they may not contain specific domain knowledge, failing to answer certain prompts or outright hallucinating. 


%------------------------------------------------

\section*{Methods}

To solve these issues, \textit{Retrieval-Augmented Generation} (RAG) is used to provide the missing knowledge to the LLM. RAG employs different techniques to retrieve information from external sources based on user’s prompt and through prompt augmentation feed the LLM sufficient information to provide informative and factually correct answer. In the survey \cite{survey}, multiple approaches to RAG are presented, highlighting three architectures: naive RAG, which analyzes the user’s prompt, retrieves the required information and appends it, letting the LLM do the rest; advanced RAG, which employs pre-retrieval and post-retrieval modifications to the prompt to make it more suitable for information retrieval and subsequent interpretation by LLM; lastly, modular RAG combines multiple approaches, using iterative prompt enhancement, ranking, fusion, etc. 


To better evaluate RAG performance, \cite{benchmark} presents the CRUD framework, employing metrics such as ROUGE, BLEU, precision and recall. Various operations on text (creative generation from context, usage of information to answer questions, identification and correction of false information, summarization, ...) are measured separately to give a more detailed overview of the model. 

For document summarization, LLMs, statistical models, graph-based models and other approaches are used to extract the most important information from text. \cite{summarization} present multiple solutions, noting that LLMs, while consuming more resources, tend to be more coherent and precise in their summarization if trained correctly. 

Recently, a novel LLM, DeepSeek \cite{deepseek3}, has been presented. Because of its positive benchmarking results and efficiency due to the small number of parameters, it provides a promising starting point for experimentation with knowledge injection and prompt engineering. 

\subsection*{Topic domain}
In this paper, we focus on implementing a conversational agent operating on knowledge about different art and media. Specifically, the agent is to suggest and converse about films and other related media on the user’s prompts and preferences. In our contributions, we: 
\begin{itemize}
\item Develop a conversational wrapper around an existing pretrained LLM, DeepSeek-R1 \footnote{https://huggingface.co/deepseek-ai/DeepSeek-R1}; 
\item Analyze and test prompt engineering techniques on LLM inputs for our defined use cases, noting the placement of information in the prompt, structuring of the prompt, and wording that produces best results; 
\item Implement an advanced and/or modular RAG to transform and enhance user prompts and inject necessary knowledge into the final prompt, using approaches such as summarization, ranking, iterative prompt enhancement, and sentiment analysis via smaller pretrained LLMs; 
\item Retrieve data from open databases, such as The\-Movie\-Database (TMDB) \footnote{https://www.themoviedb.org/}, JustWatch \footnote{https://www.justwatch.com/}, and social media platforms, e.g., Letterboxd \footnote{https://letterboxd.com/}; 
\item Perform benchmarks of our solution with CRUD framework; 
\item Compare our solution with advanced commercial LLMs, such as ChatGPT. 
\end{itemize}

\subsection*{Approach}
To develop a reliable conversational agent, we must address several challenges.
Our approach focuses on the following areas:
\begin{itemize}
\item Understanding user prompts:\\
Natural language queries are often ambiguous or incomplete, making effective information retrieval difficult.
\textit{Rephrase and Respond} (RaR) \cite{RaR} prompting refines user queries by expanding, rewording or clarifying intent, ensuring more effective retrieval and response generation.
\item Reducing factually inaccurate output:\\
One of the primary issues with LLMs is their tendency to generate misleading or incorrect information. 
To counter this, we apply \textit{Forward Looking Active Retrieval augmented generation} (FLARE) \cite{FLARE}. The method decides when and what to retrieve to improve accuracy and reduce inaccurate output.
\item Consistency:\\
Conversational agents must maintain logical coherence and avoid contradictions.
\textit{Contrastive Chain-of-Thought} (CCoT) \cite{CCoT} prompting helps models identify patterns and avoid mistakes by comparing correct and incorrect examples.
This step is crucial for conversational agents.
\item Fine-tuning and optimization:\\
To maximize efficiency and response quality, we employ
\textit{Automatic Prompt Engineer} (APE) \cite{APE} analyzes user input, generates candidate instructions and then uses reinforcement learning to choose the optimal prompt. 
\end{itemize}


\subsection*{Data scraping}

Crucial step in the RAG pipeline is the gathering of data (that the model has not been trained on) from external sources, in our case this is achieved via web scraping. Based on the user prompt, our models can access the following data:
\begin{itemize}
	\item basic information about a film or a person, obtained from TMDB
	\item a specified number of film reviews, obtained from social media platform Letterboxd, sorted by popularity
	\item a list of streaming services a specific film is available on
\end{itemize}

\subsection*{POS tagging and summarization}

For the purpose of identifying film titles or names in user's prompt, we use a Roberta-like spacy model en\_core\_web\_trf \footnote{https://huggingface.co/spacy/en\_core\_web\_trf}. We use the same model to summarize retrieved data for advanced RAG.  


%------------------------------------------------

\section*{Results}

In this section we compare the performance of a reasoning deepseek-r1 model and a non-reasoning qwen 1.5 model. For each of these we run experiments on three different sets of parameters: without RAG, with naive RAG - data is scraped and appended to the context, and advanced RAG - data is additionally processed and summarized before being inputed into the model. In total, we are evaluating the following models:
\begin{itemize}
	\item deepseek-r1 with 1.5B parameters (baseline)
	\item deepseek-r1 with 1.5B parameters and naive RAG
	\item deepseek-r1 with 1.5B parameters and advanced RAG
	\item qwen 1.5 with 1.8B parameters (baseline)
	\item qwen 1.5 with 1.8B parameters and naive RAG
	\item qwen 1.5 with 1.8B parameters and advanced RAG
\end{itemize}

Since we are working with relatively open-ended questions, there is objective ground truth to evaluate against. Therefore, we construct a set of 51 domain-relevant questions and use manually checked answers from the commercial ChatGPT model as ground truth, so we can apply standard metrics such as ROUGE and BLEU \footnote{https://huggingface.co/docs/evaluate/index}. Results are shown in Table \ref{tab:comparison}.

\begin{table}[!htb]
\centering
\resizebox{1.0 \linewidth}{!}{\begin{tabular}{| l | c c |}
\hline
% & &\multicolumn{5}{c}{Number of segmented training images}\\
Model & ROUGE & BLEU   \\ \hline
deepseek-r1 baseline  & 0.13079 & 0.00567 \\
deepseek-r1 naive  & 0.14052  & 0.01068   \\
deepseek-r1 advanced  & 0.16714  & 0.01866   \\
qwen 1.5 baseline  & 0.18991 & 0.03687    \\
qwen 1.5 naive  & 0.19899  & 0.05811  \\
qwen 1.5 advanced  & 0.17380  & 0.02584   \\
\hline
\end{tabular}}
\caption{Performance comparison against ChatGPT-generated ground truth.}
\label{tab:comparison}
\end{table}


%------------------------------------------------

\section*{Discussion}

At the moment, there is a visible improvement in the models' performance when using RAG, however the BLEU and ROUGE evaluation metrics do not emphasize this improvement enough, therefore we intend to devise more representative evaluation methods for our specific domain. Other open problems are how to determine which title/person a query is referring to in certain ambiguous cases and how to enable the model to keep a memory in the conversational context. 



%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}