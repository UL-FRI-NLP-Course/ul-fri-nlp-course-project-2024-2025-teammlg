\begin{thebibliography}{1}

\bibitem{survey}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai, Jiawei Sun, Meng Wang, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey, 2024.

\bibitem{benchmark}
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo~Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen.
\newblock Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models, 2024.

\bibitem{summarization}
Yang Zhang, Hanlei Jin, Dan Meng, Jun Wang, and Jinghua Tan.
\newblock A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods, 2025.

\bibitem{deepseek3}
DeepSeek-AI.
\newblock Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.

\bibitem{RaR}
Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu.
\newblock Rephrase and respond: Let large language models ask better questions for themselves, 2024.

\bibitem{FLARE}
Zhengbao Jiang, Frank~F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Active retrieval augmented generation, 2023.

\bibitem{CCoT}
Yew~Ken Chia, Guizhen Chen, Luu~Anh Tuan, Soujanya Poria, and Lidong Bing.
\newblock Contrastive chain-of-thought prompting, 2023.

\bibitem{APE}
Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.
\newblock Large language models are human-level prompt engineers, 2023.

\end{thebibliography}
